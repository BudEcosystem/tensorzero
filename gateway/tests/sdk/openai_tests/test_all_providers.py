"""
Test OpenAI SDK compatibility with all providers through /v1/chat/completions endpoint.

This demonstrates that the OpenAI SDK can work with any provider that supports
the chat completions endpoint, making it a universal SDK for TensorZero.
"""

import os
import pytest
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Configuration
TENSORZERO_BASE_URL = os.getenv("TENSORZERO_BASE_URL", "http://localhost:3001")
TENSORZERO_API_KEY = os.getenv("TENSORZERO_API_KEY", "test-api-key")

# Universal OpenAI client that works with all providers
client = OpenAI(
    base_url=f"{TENSORZERO_BASE_URL}/v1",
    api_key=TENSORZERO_API_KEY
)


class TestOpenAISDKUniversalCompatibility:
    """Test OpenAI SDK with all provider models through /v1/chat/completions."""
    
    def test_openai_models(self):
        """Test OpenAI SDK with OpenAI models."""
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello from OpenAI model"}],
            max_tokens=50
        )
        
        assert response.choices[0].message.content is not None
        assert response.model == "gpt-3.5-turbo"
        assert len(response.choices[0].message.content) > 0
    
    def test_anthropic_models_via_openai_sdk(self):
        """Test OpenAI SDK with Anthropic models through /v1/chat/completions."""
        response = client.chat.completions.create(
            model="claude-3-haiku-20240307",
            messages=[{"role": "user", "content": "Hello from Anthropic model via OpenAI SDK"}],
            max_tokens=50
        )
        
        assert response.choices[0].message.content is not None
        assert response.model == "claude-3-haiku-20240307"
        assert len(response.choices[0].message.content) > 0
    
    def test_anthropic_claude_35_sonnet(self):
        """Test OpenAI SDK with Claude 3.5 Sonnet."""
        response = client.chat.completions.create(
            model="claude-3-5-sonnet-20241022",
            messages=[{"role": "user", "content": "Hello from Claude 3.5 Sonnet"}],
            max_tokens=50
        )
        
        assert response.choices[0].message.content is not None
        assert response.model == "claude-3-5-sonnet-20241022"
        assert len(response.choices[0].message.content) > 0
    
    def test_anthropic_opus(self):
        """Test OpenAI SDK with Claude Opus."""
        response = client.chat.completions.create(
            model="claude-3-opus-20240229",
            messages=[{"role": "user", "content": "Hello from Claude Opus"}],
            max_tokens=50
        )
        
        assert response.choices[0].message.content is not None
        assert response.model == "claude-3-opus-20240229"
        assert len(response.choices[0].message.content) > 0
    
    def test_streaming_with_all_providers(self):
        """Test streaming compatibility across different providers."""
        # Test with OpenAI model
        stream = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": "Count to 3"}],
            max_tokens=30,
            stream=True
        )
        
        chunks = []
        for chunk in stream:
            if chunk.choices[0].delta.content:
                chunks.append(chunk.choices[0].delta.content)
        
        assert len(chunks) > 0
        
        # Test with Anthropic model
        stream = client.chat.completions.create(
            model="claude-3-haiku-20240307",
            messages=[{"role": "user", "content": "Count to 3"}],
            max_tokens=30,
            stream=True
        )
        
        chunks = []
        for chunk in stream:
            if chunk.choices[0].delta.content:
                chunks.append(chunk.choices[0].delta.content)
        
        assert len(chunks) > 0
    
    def test_system_prompts_cross_provider(self):
        """Test system prompts work across different providers."""
        system_prompt = "You are a helpful assistant. Always start responses with 'Assistant:'"
        
        # Test with OpenAI
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "Hello"}
            ],
            max_tokens=50
        )
        
        assert response.choices[0].message.content is not None
        
        # Test with Anthropic  
        response = client.chat.completions.create(
            model="claude-3-haiku-20240307",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "Hello"}
            ],
            max_tokens=50
        )
        
        assert response.choices[0].message.content is not None
    
    def test_temperature_parameter_cross_provider(self):
        """Test temperature parameter works across providers."""
        for model in ["gpt-3.5-turbo", "claude-3-haiku-20240307"]:
            # Low temperature
            response1 = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "What is 2+2?"}],
                max_tokens=20,
                temperature=0.0
            )
            
            # High temperature
            response2 = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "What is 2+2?"}],
                max_tokens=20,
                temperature=1.0
            )
            
            assert response1.choices[0].message.content is not None
            assert response2.choices[0].message.content is not None
    
    def test_multi_turn_conversation_cross_provider(self):
        """Test multi-turn conversations work across providers."""
        for model in ["gpt-3.5-turbo", "claude-3-haiku-20240307"]:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": "My name is Alice"},
                    {"role": "assistant", "content": "Hello Alice! Nice to meet you."},
                    {"role": "user", "content": "What's my name?"}
                ],
                max_tokens=50
            )
            
            assert response.choices[0].message.content is not None
            # For dummy provider, just verify we got a response
            assert len(response.choices[0].message.content) > 0


class TestOpenAISDKEndpointCoverage:
    """Test that OpenAI SDK works with all supported endpoints across providers."""
    
    def test_chat_completions_universal(self):
        """Chat completions should work with all chat-capable models."""
        chat_models = [
            "gpt-3.5-turbo",
            "gpt-4", 
            "claude-3-haiku-20240307",
            "claude-3-sonnet-20240229",
            "claude-3-5-sonnet-20241022",
            "claude-3-opus-20240229"
        ]
        
        for model in chat_models:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": f"Testing {model}"}],
                max_tokens=20
            )
            
            assert response.choices[0].message.content is not None
            assert response.model == model
    
    def test_embeddings_openai_models(self):
        """Embeddings should work with OpenAI embedding models."""
        embedding_models = [
            "text-embedding-ada-002",
            "text-embedding-3-small", 
            "text-embedding-3-large"
        ]
        
        for model in embedding_models:
            response = client.embeddings.create(
                model=model,
                input="Test embedding text"
            )
            
            assert len(response.data) > 0
            assert len(response.data[0].embedding) > 0
            assert response.model == model
    
    def test_moderation_openai_models(self):
        """Moderation should work with OpenAI moderation models."""
        moderation_models = [
            "text-moderation-latest",
            "omni-moderation-latest"
        ]
        
        for model in moderation_models:
            response = client.moderations.create(
                model=model,
                input="This is a test message"
            )
            
            assert len(response.results) > 0
            assert hasattr(response.results[0], 'flagged')
            assert response.model == model
    
    def test_audio_openai_models(self):
        """Audio endpoints should work with OpenAI audio models."""
        # Text-to-speech
        response = client.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input="Hello world"
        )
        
        assert response.content is not None
        assert len(response.content) > 0
    
    def test_image_generation_openai_models(self):
        """Image generation should work with OpenAI image models."""
        image_models = ["dall-e-2", "dall-e-3"]
        
        for model in image_models:
            response = client.images.generate(
                model=model,
                prompt="A simple test image",
                n=1,
                size="256x256"
            )
            
            assert len(response.data) > 0
            assert response.data[0].url is not None or response.data[0].b64_json is not None


class TestOpenAISDKVsNativeSDK:
    """Compare OpenAI SDK usage vs native SDK usage for the same provider."""
    
    def test_anthropic_comparison_demo(self):
        """Demonstrate that Anthropic models work with OpenAI SDK."""
        # Using OpenAI SDK with Anthropic model through /v1/chat/completions
        openai_sdk_response = client.chat.completions.create(
            model="claude-3-haiku-20240307",
            messages=[{"role": "user", "content": "Hello from OpenAI SDK"}],
            max_tokens=50
        )
        
        # Verify the response
        assert openai_sdk_response.choices[0].message.content is not None
        assert openai_sdk_response.model == "claude-3-haiku-20240307"
        
        # Note: Native Anthropic SDK would use /v1/messages endpoint
        # Both approaches work, but OpenAI SDK provides universal compatibility
    
    def test_openai_model_native_usage(self):
        """Demonstrate OpenAI models work naturally with OpenAI SDK."""
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello from native OpenAI usage"}],
            max_tokens=50
        )
        
        assert response.choices[0].message.content is not None
        assert response.model == "gpt-3.5-turbo"


if __name__ == "__main__":
    # Allow running tests directly
    pytest.main([__file__, "-v"])